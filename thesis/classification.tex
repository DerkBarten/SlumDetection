\section{Classification}

% TODO: continue creating intro
%In the evaluation of the effectiveness of the features in classification



\subsection{Classification Algorithms}
In order to discover the optimal method for classification of our features, we use a set of different supervised learning algorithms. These algorithms are Decision Tree, Random Forrest, AdaBoosting, GradientBoosting and MLP classifier. We use Decision Trees because this would produce a model which would be easy to interpret. However, the disadvantage of Decision Trees is the general inaccuracy and instability to small changes in data. To compensate for the disadvantages of the Decision Tree classifier, we use a Random Forrest as wel. A Random Forrest is a form of ensamble learning, which combines multiple learning algorithms to improve predictive performance. The Random Forrest constructs and uses multiple Decision Trees for classification. This generally reduces the tendency of single Decision Trees to overfit. Similar to Random Forrest, AdaBoosting and GradientBoosting are both ensemble leraning techniques. Both use a set of weak learning algorithms to create a single strong learner. AdaBoost, especially, is known to perform well without the need to adjust many parameters. The MLP  or Multi Layer Perceptron classifier is a class of feedforward artificial neural network. The MLP is a fairlt uncomplex neural network that is used for training and classification of data. The hidden layer allows the MLP to classify data that cannot linearly be separated. We used scikit-learn package for the implementation of these classification algorithms \cite{scikit-learn}.

\subsection{Performance Measures}
We require a objective measure of performance to compare the results between different features, parameters and images. Measuring classification performance on highly imbalanced datasets is difficult. Common measures, like accuracy, often failt to capture the actual performance. For instance, on highly imbalanced data, classifying everything as the most common class could result in high accuracy, although the classification is completely trivial and useless. To get a more useful impression of the results, we use to metrics to measure the performance. These two are the F1-Score and Matthews coefficient. The F1 gives a measure of performance based on the balance between precision and recall. A shortcoming of F1 score is that it does not take true negatives into account. We compensate for this by using matthews correlation coefficient. As the F1 score, matthews coefficient is a measure quality in binary classification. It is known to produce balanced quality measures even when the classes are of vastly different sizes.

\subsection{Data Preparation}

For the classification, we used the three sections displayed in Figure \ref{fig:sections}. These sections of the image contain the most largest concentration of slums in the image. We divided the the three sections as two sections for training and one section for testing. Although these regions contain the highest concentrations of slums, there is still a significant class imbalance between the two classes. The ratio of formal to informal surface area is often 100 to 1 or more larger. Classification algorithms usually do not perform well on imbalanced data, since they are likely to classify everything as the most common class. There are multiple approaches to balance the dataset, such as undersampling, oversampling and synthethic oversampling. Undersampling removes occurences of the most common class, oversampling duplicates existing data points of the minority class. Synthethic oversampling adds new data points based on existing data points of the minority class. We use a synthetic oversampling technique called SMOTE. We will evaluate the effect of these different methods on the performance of the classifiers. Besides imbalanced data, classification algorithms may encounter difficulties when the ranges of values within the features are not balanced. Therefore, we scale the values of the features to a range of negative one to positive one.

\subsection{Experimental Setup}

In this section we discuss the experimental setup for feature classification experimetns in the next section. As mentioned in the previous subsection, we use combinations of the three sections in the creation of the test and training sets. The division of the test and training sets is 1 to 2; a single section as test image, the other sections as training images. We used a whole section as test set as this allows to overlay of the predicted slums over the original image, which could help to identify parts of the image that are difficult to classify correctly by means of visual inspection. To discover the effects of the test image on performance, we will evaluate the performance of all feature sets on each section as test image

These feature sets are: LSR, HoG, RID combined in a single set, together with the three features in separate sets, resulting in four different feature sets. Using the features separate and in combination enables us to observe the individual performance features and infer what features are performing well for what test images.

Besides discovering the effects of test image and features of performance, we will also evaluate the impact of scale and block size on the classification performance. In these two experiments, the only variable is the scale and block size, which allows us to conclude about the impact of these parameters on the performance. In all other experiments, the scale and block size will have a constant value, which is displayed in Figure \ref{fig:params}.


As explained in the data preparation section, using a balanced dataset is essential for the performance of classification algorithms. In the experiments described above, we use the SMOTE oversampling method to balance the dataset. The SMOTE method performed well although other oversampling options were not thoroughly explored. We therefore create a separate experiment to discover the effects of various oversampling methods on the classification performance and compare this to the performance without oversampling. 

Lastly, to reduce the impact of randomness from the classifiers, the measured performance is the mean of five repeats of the experiments.

\begin{figure}
	\centering
	\begin{tabular}{|ll|}
		\hline
		Test Images: & Section 1, 2, and 3 \\
		Features: & HoG, LSR, RID, All three features\\
		Scales: & 50, 100 and 150 \\
		Block size: & 20\\
		Classifiers: & Decision Tree, Random Forrest, AdaBoost, GradientBoost, MLP \\
		Performance Metrics: & Precision, F1-Score, Jaccard's Index\\
		Oversampling Method: & SMOTE\\
		\hline
	\end{tabular}
	\caption{The noteworthy parameters of the classification evaluation}
	\label{fig:params}
\end{figure}

