\section{Classification}

\subsection{Classification Algorithms}
To discover the best method of classification for our features, we use a set of different supervised learning algorithms. These algorithms are Decision Tree, Random Forrest, AdaBoost, Gradient boosting and MLP classifier. We use Decision Trees because this would produce a model which would be easy to interpret. However, the disadvantage of Decision Trees is the general inaccuracy and instability of small changes in data. To compensate for the disadvantages of the Decision Tree classifier, we used the Random Forrest classifier as well, which is a form of ensemble learning that combines multiple weak learning algorithms to improve predictive performance. In the case of the Random Forrest, it uses Decision Trees to construct a strong classification algorithm that reduces the tendency of single Decision Trees to overfit. Similar to Random Forrest, AdaBoost and Gradient boosting are both ensemble learning techniques as well; AdaBoost especially is known to perform well without the need to adjust many parameters. The MLP  or Multi-Layer Perceptron classifier is a class of feedforward artificial neural network that is used for training and classification using a hidden layer and can classify non-linear relationships in the data. We used the scikit-learn package for the implementation of these classification algorithms \cite{scikit-learn}.

\subsection{Performance Measures}
We require an objective measure of performance to compare the results between different features, parameters, and images. Measuring classification performance on highly imbalanced datasets is difficult; common measures, like accuracy, often fail to capture the actual performance. For instance, on highly imbalanced data, classifying everything as the most common class could result in high accuracy, although the classification is completely trivial and useless. To get a more useful impression of the results, we used the F1-score and Matthews correlation coefficient as performance metrics. The F1-score gives a measure of performance based on the balance between precision and recall. A shortcoming of F1-score is the disregard of true negatives. We compensated for this by using Matthews coefficient. Similar to the F1-score, the Matthews coefficient is a measure of quality in binary classification that is known to produce balanced quality measures regardless of vastly different class sizes.

\subsection{Data Preparation}

In the classification, we use the three sections displayed in Figure \ref{fig:sections}. These sections of the image contain the largest concentration of slums in the image. We divided the three sections into two sections for training and one section for testing. Although these regions of the image contain the highest concentrations of slums, there is still a significant class imbalance; the ratio of formal to informal surface area is often 100 to 1 or more. Classification algorithms generally do not perform well on imbalanced data as they are likely to classify everything as the most common class. Two standard approaches to balance the dataset are undersampling and oversampling. Undersampling removes occurrences of the most common class while oversampling duplicates existing data points of the minority class. We will use SMOTEBoost, which is a form of synthetic oversampling that creates new data points based on the minority class \cite{chawla2003smoteboost}. Besides imbalanced data, classification algorithms may encounter difficulties when the ranges of values within the features are not balanced. We, therefore, scale the values of the features to a range of negative one to positive one.

\subsection{Experimental Setup}

In this section, we discuss the experimental setup for feature classification experiments in the next section. The division of the test and training sets is 1 to 2; a single section is the test image while the other sections are training images. We use a single section as the test set because this allows displaying the predicted slums over the original image, which could help to identify parts of the image that are difficult to classify correctly through visual inspection. To discover the effects of the test image on performance, we will evaluate the performance of all feature sets on each section as the test image. These feature sets are: LSR, HoG, and RID combined in a single set, together with the three features in separate sets, resulting in four different feature sets. Using the features separate and in combination enables us to observe the individual performance features and infer what features are performing well for what test images.

Besides discovering the effects of the test image and features of performance, we will also evaluate the impact of scale and block size on the classification performance. In these two experiments, the only variable is the scale and block size, which allows us to conclude the impact of these parameters on the performance. In all other experiments, the scale and block size will have a constant value, which is displayed in Figure \ref{fig:params}.

As explained in the data preparation section, using a balanced dataset is essential for the performance of classification algorithms. In the experiments described above, we use the SMOTEBoost oversampling method to balance the dataset as we experienced that performed well although alternative oversampling methods were not thoroughly explored.  We perform a separate experiment to discover the effects of oversampling methods on the classification performance and compare this to the performance of the dataset without oversampling. Besides SMOTEBoost, these different oversampling methods are an unsophisticated random oversampler and ADASYN, which is a synthetic oversampling method \cite{he2008adasyn}. These dataset balancing methods are implemented in the Python package Imbalanced-learn \cite{lemaitre2017imbalanced}.

The classifiers that are used contain random components, therefore changing performance for every classification on a constant dataset. The presented performance in the experiments is the mean of five repeats of each experiment to reduce the impact of randomness. The repetition of experiments should provide a more accurate picture of the performance than a single experiment.

\begin{figure}
	\centering
	\begin{tabular}{|ll|}
		\hline
		Test Images: & Section 1, 2, and 3 \\
		Features: & HoG, LSR, RID, All three features\\
		Scales: & 50, 100 and 150 \\
		Block size: & 20\\
		Classifiers: & Decision Tree, Random Forrest, AdaBoost, Gradient boosting, MLP \\
		Performance Metrics: & F1-Score, Matthews coefficient\\
		Oversampling Method: & SMOTE\\
		\hline
	\end{tabular}
	\caption{The main parameters of the classification experiments}
	\label{fig:params}
\end{figure}

